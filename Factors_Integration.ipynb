{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Factors Integration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ed74919b6a2e45a78df6ce6dde84b90d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1861fbfa7dcd4647968e3e436ed6a53c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6355f512382a415ea8fbabc3158e1f01",
              "IPY_MODEL_37cf55465fca4f0480c70937833e9a1d"
            ]
          }
        },
        "1861fbfa7dcd4647968e3e436ed6a53c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6355f512382a415ea8fbabc3158e1f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7834885ab3314839859e6493be058273",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bb1e21bd19f2463fa956e43b7661f37c"
          }
        },
        "37cf55465fca4f0480c70937833e9a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_86b8d0ad0e884214b5187990a56e98a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:06&lt;00:00, 70.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc123a22ff424b598e3fbb944e70f500"
          }
        },
        "7834885ab3314839859e6493be058273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bb1e21bd19f2463fa956e43b7661f37c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "86b8d0ad0e884214b5187990a56e98a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc123a22ff424b598e3fbb944e70f500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98f3cd523e234fb3ac902daf52cad8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f592b47267424c00b0e1eb632b171b3a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d775d23b7235439a8f6634c521beb703",
              "IPY_MODEL_4dc2a3dc5fc84300b189b615e8f13111"
            ]
          }
        },
        "f592b47267424c00b0e1eb632b171b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d775d23b7235439a8f6634c521beb703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_26aa20d7ce0f4876ab4542eb652487ed",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_16a280df14314c069ee6cc92161ffacf"
          }
        },
        "4dc2a3dc5fc84300b189b615e8f13111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8eee28bd92c14a30b69b62be7f24ec1c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:05&lt;00:00, 77.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4a84e4dcb40a4c8483507854ceb37dd4"
          }
        },
        "26aa20d7ce0f4876ab4542eb652487ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "16a280df14314c069ee6cc92161ffacf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8eee28bd92c14a30b69b62be7f24ec1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4a84e4dcb40a4c8483507854ceb37dd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4df99dedcb064dad9ee0a0391b6cd476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_32e1fc398ad54458bb881b54f1e9200e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8c528c25930f4c06ad96f231a9a5b665",
              "IPY_MODEL_1673b67d1f984586a32de449afc33fb1"
            ]
          }
        },
        "32e1fc398ad54458bb881b54f1e9200e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c528c25930f4c06ad96f231a9a5b665": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8eca7f764862415b9ca757a4dc9cc404",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96b61f233da4470d99253697ca50a26a"
          }
        },
        "1673b67d1f984586a32de449afc33fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b2fdd8d4dce44cafb146256900b1630a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 824kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f37bac73a994f2795371adc534f97ef"
          }
        },
        "8eca7f764862415b9ca757a4dc9cc404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96b61f233da4470d99253697ca50a26a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b2fdd8d4dce44cafb146256900b1630a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f37bac73a994f2795371adc534f97ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roanacla/FactorsIntegration/blob/main/Factors_Integration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXE3p9Gc8uVt"
      },
      "source": [
        "# Clone repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aScZNbVH-btB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4175def-4843-42ed-939a-c473b59d044c"
      },
      "source": [
        "#Content Statistics\n",
        "!git clone https://github.com/Wayne122/Content_Statistics_Demo.git\n",
        "%cd /content/Content_Statistics_Demo\n",
        "!pip install -r requirements.txt\n",
        "!python3 -m spacy download en_core_web_lg\n",
        "\n",
        "#Sensationalism\n",
        "%cd /content/\n",
        "!git clone https://github.com/roanacla/nlp_sensationalism.git\n",
        "!pip install -r /content/nlp_sensationalism/requirements.txt\n",
        "\n",
        "#Context Veracity\n",
        "%cd /content/\n",
        "!git clone https://github.com/snarvekark/Veracity_Factor.git\n",
        "%cd /content/Veracity_Factor/\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Content_Statistics_Demo'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 31 (delta 15), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (31/31), done.\n",
            "/content/Content_Statistics_Demo\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (1.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (0.17.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 4)) (1.15.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.0.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (1.0.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (50.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->-r requirements.txt (line 5)) (2.0.4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->-r requirements.txt (line 5)) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->-r requirements.txt (line 5)) (3.4.0)\n",
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=af74a1248d72cc4530aebfe18cbb84e5320ee464b8d2ee8b5b085ec3ec2c54f8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qgup11x4/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "/content\n",
            "Cloning into 'nlp_sensationalism'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 19 (delta 7), reused 16 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from -r /content/nlp_sensationalism/requirements.txt (line 2)) (0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (20.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2.23.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 25.4MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->-r /content/nlp_sensationalism/requirements.txt (line 1)) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=5f7bf40d14b98c409df79c4dd1bce80229270de4de296a3ee9aa1a5f8759872a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n",
            "/content\n",
            "Cloning into 'Veracity_Factor'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Counting objects: 100% (57/57), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 57 (delta 25), reused 40 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n",
            "/content/Veracity_Factor\n",
            "Collecting git+https://github.com/abenassi/Google-Search-API (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/abenassi/Google-Search-API to /tmp/pip-req-build-fswxt7h8\n",
            "  Running command git clone -q https://github.com/abenassi/Google-Search-API /tmp/pip-req-build-fswxt7h8\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (3.2.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (4.6.3)\n",
            "Collecting selenium<3.0.0,>=2.44.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/f8/e46684764161fee7b7d2e0fd9b82d56915438b68e498181dc45183643c82/selenium-2.53.6-py2.py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (2.23.0)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 23.8MB/s \n",
            "\u001b[?25hCollecting vcrpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/62/571e9fa5c2a2c986c001d1be99403a5e800d2e72b905e6b1e951148c75c9/vcrpy-4.1.1-py2.py3-none-any.whl (40kB)\n",
            "\u001b[K     |████████████████████████████████| 40kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (0.16.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from vcrpy->-r requirements.txt (line 6)) (3.13)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from vcrpy->-r requirements.txt (line 6)) (1.12.1)\n",
            "Collecting yarl; python_version >= \"3.6\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 28.3MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 9)) (2.8.1)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/62/b6acd3129c5615b9860e670df07fd55b76175b63e6b7f68282c7cad38e9e/tldextract-3.1.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 11.8MB/s \n",
            "\u001b[?25hCollecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 9)) (7.0.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k->-r requirements.txt (line 9)) (4.2.6)\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.1MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 29.6MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->-r requirements.txt (line 10)) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from yarl; python_version >= \"3.6\"->vcrpy->-r requirements.txt (line 6)) (3.7.4.3)\n",
            "Collecting multidict>=4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/95/f89c97db4d27b24fce7c6fb6ef05228f347e43bbf67e4737a0660c8753fd/multidict-5.0.2-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k->-r requirements.txt (line 9)) (3.0.12)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (0.17.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 10)) (1.18.5)\n",
            "Building wheels for collected packages: fake-useragent, Google-Search-API, tinysegmenter, jieba3k, feedfinder2, sgmllib3k\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=a350f508f8e8615eedc2eddc38f84f72a039cdfd1c7d4e1a75a836b0ccf908e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for Google-Search-API (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Google-Search-API: filename=Google_Search_API-1.1.14-cp36-none-any.whl size=607835 sha256=76ea6dae72e096d57fd718e81a0328e52a1d960241beffcc72c4f458496880cd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a_jjvtss/wheels/9d/dd/52/f744758aa94909328835af5d6f1650c9c3d8cf2e6e8988767a\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13538 sha256=bc1cd6562daee1d610d32a257af8ad6ad8860995c8fa27d06bf73236c8078e64\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=a83862c74aca8f38380b09b9f8a0324b79bb026112368a3fad65cb25ebe899d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3355 sha256=2b48f555ceca176806719cdf03302e3768d385a96db8b3f702b6e2f8ebbe3236\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp36-none-any.whl size=6067 sha256=63d290e1255d19167d1e3d3473528a563182541fc436ba8cf09c19a380c76d10\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built fake-useragent Google-Search-API tinysegmenter jieba3k feedfinder2 sgmllib3k\n",
            "Installing collected packages: selenium, unidecode, multidict, yarl, vcrpy, fake-useragent, cssselect, requests-file, tldextract, tinysegmenter, sgmllib3k, feedparser, jieba3k, feedfinder2, newspaper3k, Google-Search-API\n",
            "Successfully installed Google-Search-API-1.1.14 cssselect-1.1.0 fake-useragent-0.1.11 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 multidict-5.0.2 newspaper3k-0.2.8 requests-file-1.5.1 selenium-2.53.6 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0 unidecode-1.1.1 vcrpy-4.1.1 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of7JiFLhNVko"
      },
      "source": [
        "Note: After running the above cell, please restart the runtime. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwB3Wm2iAdHe"
      },
      "source": [
        "# Instantiation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sB9WH4E5AKtB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644,
          "referenced_widgets": [
            "ed74919b6a2e45a78df6ce6dde84b90d",
            "1861fbfa7dcd4647968e3e436ed6a53c",
            "6355f512382a415ea8fbabc3158e1f01",
            "37cf55465fca4f0480c70937833e9a1d",
            "7834885ab3314839859e6493be058273",
            "bb1e21bd19f2463fa956e43b7661f37c",
            "86b8d0ad0e884214b5187990a56e98a1",
            "fc123a22ff424b598e3fbb944e70f500",
            "98f3cd523e234fb3ac902daf52cad8a5",
            "f592b47267424c00b0e1eb632b171b3a",
            "d775d23b7235439a8f6634c521beb703",
            "4dc2a3dc5fc84300b189b615e8f13111",
            "26aa20d7ce0f4876ab4542eb652487ed",
            "16a280df14314c069ee6cc92161ffacf",
            "8eee28bd92c14a30b69b62be7f24ec1c",
            "4a84e4dcb40a4c8483507854ceb37dd4"
          ]
        },
        "outputId": "f64e2b29-ee7b-41b4-c6e6-b57e7188d473"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Content Statistics\n",
        "%cd /content/Content_Statistics_Demo/\n",
        "from Blastoff_Content_Statistics import BlastoffContentStatistics\n",
        "%cd /content/\n",
        "bcs = BlastoffContentStatistics()\n",
        "\n",
        "#Sensationalims\n",
        "from nlp_sensationalism.sensaScorer import SensaScorer\n",
        "sensa = SensaScorer()\n",
        "\n",
        "#Context Veracity\n",
        "%cd /content/Veracity_Factor/\n",
        "from context_veracity import Context_Veracity\n",
        "cv = Context_Veracity()\n",
        "\n",
        "%cd /content/"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Content_Statistics_Demo\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "/content\n",
            "Downloading 1oTfsNgkmEBemkVfrWSjnc-K9svmL7Iak into ./bcs_encoder.zip... Done.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Loading default pretrained model.\n",
            "Downloading 1oFjoL9LWrp2-YPSJL2UhBQ1efV9LiC2n into ./content_statistic_model.pickle... Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed74919b6a2e45a78df6ce6dde84b90d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98f3cd523e234fb3ac902daf52cad8a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading 1SpfmiCq2a2aXTXvFW6cHnm-0eBCpcyxY into ./sensationalism_BERT_best.model... Done.\n",
            "/content/Veracity_Factor\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Downloading 18NyBuNHikHiUzrAC4oOMUOO5KAaKouEY into ./context_veracity_models.zip... Done.\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Wb2vWNrAnbW"
      },
      "source": [
        "# Run Polinomial Equation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5s4Zc5783eW"
      },
      "source": [
        "def isFake(text):\n",
        "  accur = [0.26, 0.80, 0.50]\n",
        "  w = [float(i)/sum(accur) for i in accur]\n",
        "  prob = []\n",
        "\n",
        "  if text:\n",
        "    in_df = pd.DataFrame(data=[text], columns=['Statement'])\n",
        "    res_cs = bcs.predict(in_df).replace('pants-fire', 1.0).replace('false', 0.8).replace('barely-true', 0.6).replace('half-true', 0.4).replace('mostly-true', 0.2).replace('true', 0.0)\n",
        "    prob.append(w[0] * float(res_cs.iloc[0]))\n",
        "    \n",
        "  if text:\n",
        "    cv_val = cv.get_veracity_scores(text)\n",
        "    if cv_val == 1:\n",
        "      cv_final = 0.8\n",
        "    elif cv_val == 2:\n",
        "      cv_final = 0.4\n",
        "    elif cv_val == 3:\n",
        "      cv_final = 0.2\n",
        "    elif cv_val == 4:\n",
        "      cv_final = 1.0\n",
        "    elif cv_val == 5:\n",
        "      cv_final = 0.0\n",
        "    elif cv_val == 0:\n",
        "      cv_final = 0.6\n",
        "    prob.append(w[1] * float(cv_final))\n",
        "    \n",
        "  if text:\n",
        "    prob.append(w[2] * 0.75 * sensa.getScore(text))\n",
        "    \n",
        "\n",
        "  probTotal = sum(prob[0:len(prob)])\n",
        "  return probTotal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92TztHp2csyv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eaa1803-fe2b-43a5-be26-42ece7b35c53"
      },
      "source": [
        "# result = isFake(\"Smartphones will become real personal computers\")\n",
        "# result = isFake(\"One America News Network, one of President Trump’s favorite media outlets, has been banned from posting new videos to YouTube for a week for spreading Covid-19 misinformation, YouTube said on Tuesday.\")\n",
        "result = isFake(\"Commander of the US Army: The elite of the US military will encircle China like the Soviet Union\")\n",
        "print(\"\\n\\n###### Result ######\")\n",
        "print(result)\n",
        "#if result > 0.5:\n",
        "#  print(\"FAKE NEWS ALERT!!!\")\n",
        "#else:\n",
        "#  print(\"NOT FAKE\")\n",
        "\n",
        "#Result with 6 labels\n",
        "if(result>0.8):\n",
        "  print(\"pants-on-fire\")\n",
        "elif(result<=0.80 and result>=0.64):\n",
        "  print(\"false\")\n",
        "elif(result <0.64 and result >=0.48):\n",
        "  print(\"barely-true\")\n",
        "elif(result<0.48 and result>=0.32):\n",
        "  print(\"half-true\")\n",
        "elif(result<0.32 and result>=(0.16)):\n",
        "  print('mostly-true')\n",
        "else:\n",
        "  print('true')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "###### Result ######\n",
            "0.46426282051282053\n",
            "half-true\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2142: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn8WEIUt6s4L"
      },
      "source": [
        "# Multi-modal model (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJsmILMq7FBv"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64NzeyZY7G6j"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=48, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBC9xY-lTRUy"
      },
      "source": [
        "##Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8YFFW3Z-4kN"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1sFMftHLILNfuS0kpTlsbUzWpSNs3WkeD\"})\n",
        "downloaded.GetContentFile('liar_plus_dataset.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQfX4A6S-4k2"
      },
      "source": [
        "from zipfile import ZipFile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw3M_EGd-4lC"
      },
      "source": [
        "with ZipFile('liar_plus_dataset.zip', 'r') as myzip:\n",
        "    train_data = myzip.open('train2.tsv')\n",
        "    test_data = myzip.open('test2.tsv')\n",
        "    valid_data = myzip.open('val2.tsv')\n",
        "\n",
        "train_p_df = pd.read_csv(train_data, sep='\\t', header=None).drop([0], axis=1).dropna(how='all')\n",
        "test_p_df = pd.read_csv(test_data, sep='\\t', header=None).drop([0], axis=1).dropna(how='all')\n",
        "valid_p_df = pd.read_csv(valid_data, sep='\\t', header=None).drop([0], axis=1).dropna(how='all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWWRQTPGTU0_"
      },
      "source": [
        "## Rename the columns\n",
        "\n",
        "as the module requires the data to have \"Statement\" and \"Label\" to train. If you just want to inference, then \"Statement\" is enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yoFc0jqQZp-"
      },
      "source": [
        "train_p_df.rename(columns={2:'Label', 3:'Statement'}, inplace=True)\n",
        "test_p_df.rename(columns={2:'Label', 3:'Statement'}, inplace=True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbEHKIfiQ7xG"
      },
      "source": [
        "X_train = train_p_df.drop('Label', axis=1)\n",
        "y_train = train_p_df['Label']\n",
        "X_test = test_p_df.drop('Label', axis=1)  \n",
        "y_test = test_p_df['Label']               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4ArIeZLTJbf"
      },
      "source": [
        "X_train = X_train.rename(columns={9: 'barelytruecounts',\t10: 'falsecounts',\t11: 'halftruecounts', \t12: 'mostlytruecounts', \t13: 'pantsonfirecounts'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0gY5Z67HUEM",
        "outputId": "ec0804c2-dee8-4ea9-96ea-89d2bb800a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "4df99dedcb064dad9ee0a0391b6cd476",
            "32e1fc398ad54458bb881b54f1e9200e",
            "8c528c25930f4c06ad96f231a9a5b665",
            "1673b67d1f984586a32de449afc33fb1",
            "8eca7f764862415b9ca757a4dc9cc404",
            "96b61f233da4470d99253697ca50a26a",
            "b2fdd8d4dce44cafb146256900b1630a",
            "3f37bac73a994f2795371adc534f97ef"
          ]
        }
      },
      "source": [
        "from nlp_sensationalism.sensaEncoder import SensaEncoder\n",
        "sensaEncoder = SensaEncoder()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4df99dedcb064dad9ee0a0391b6cd476",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQj6qH9C9t2a",
        "outputId": "cf05d9f8-cd78-4144-96ed-47e20d2c48c1"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "# Content Statistics\n",
        "bcs_e_X_train = bcs.encode(X_train)\n",
        "\n",
        "# Context Veracity\n",
        "bcv_e_X_train = cv.liar_encode(X_train)\n",
        "\n",
        "# Sensationalism\n",
        "bse_e_X_train = sensaEncoder.getEncodedLiarLiar() #return Dataframe already\n",
        "# sensaEncoder.encodeText(<text>,<dimension>)#Use this to encode a title\n",
        "\n",
        "# Concatenate\n",
        "bcs_e_X_train = pd.DataFrame(data=bcs_e_X_train)\n",
        "bcv_e_X_train = pd.DataFrame(data=bcv_e_X_train)\n",
        "\n",
        "e_X_train = pd.concat([bcs_e_X_train, bcv_e_X_train, bse_e_X_train], axis=1) ## Also add it here\n",
        "\n",
        "# label\n",
        "d_y_train = pd.get_dummies(y_train)\n",
        "e_X_test = bcs.encode(X_test)      \n",
        "d_y_test = pd.get_dummies(y_test)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1Pu0D6GffO5fBgXVCVnKcEAPr9lrbAYfK into ./bcv_encoder.zip... Done.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "1PsJoRjBiHrC",
        "outputId": "c9069713-6922-4d92-b377-11c397605156"
      },
      "source": [
        "e_X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "      <th>d</th>\n",
              "      <th>e</th>\n",
              "      <th>f</th>\n",
              "      <th>g</th>\n",
              "      <th>h</th>\n",
              "      <th>i</th>\n",
              "      <th>j</th>\n",
              "      <th>k</th>\n",
              "      <th>l</th>\n",
              "      <th>m</th>\n",
              "      <th>n</th>\n",
              "      <th>o</th>\n",
              "      <th>p</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.211837</td>\n",
              "      <td>4.940129</td>\n",
              "      <td>6.629362</td>\n",
              "      <td>9.050505</td>\n",
              "      <td>7.779743</td>\n",
              "      <td>5.332962</td>\n",
              "      <td>7.848826</td>\n",
              "      <td>2.241910</td>\n",
              "      <td>4.794161</td>\n",
              "      <td>4.960108</td>\n",
              "      <td>7.893267</td>\n",
              "      <td>10.353553</td>\n",
              "      <td>3.282789</td>\n",
              "      <td>6.091529</td>\n",
              "      <td>6.585247</td>\n",
              "      <td>7.080389</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.090829</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.522613</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.261555</td>\n",
              "      <td>1.204743</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.557130</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.858425</td>\n",
              "      <td>-0.145043</td>\n",
              "      <td>-0.360949</td>\n",
              "      <td>-0.117985</td>\n",
              "      <td>0.161849</td>\n",
              "      <td>-0.182158</td>\n",
              "      <td>0.692266</td>\n",
              "      <td>0.390122</td>\n",
              "      <td>0.010706</td>\n",
              "      <td>0.128772</td>\n",
              "      <td>0.126779</td>\n",
              "      <td>-0.362850</td>\n",
              "      <td>-0.146944</td>\n",
              "      <td>-0.208223</td>\n",
              "      <td>-0.495680</td>\n",
              "      <td>-0.603125</td>\n",
              "      <td>0.482074</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.685092</td>\n",
              "      <td>24.993542</td>\n",
              "      <td>8.810983</td>\n",
              "      <td>15.461455</td>\n",
              "      <td>18.635612</td>\n",
              "      <td>17.006910</td>\n",
              "      <td>12.319558</td>\n",
              "      <td>12.790240</td>\n",
              "      <td>13.600662</td>\n",
              "      <td>16.159721</td>\n",
              "      <td>24.423901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.217362</td>\n",
              "      <td>10.460907</td>\n",
              "      <td>7.305364</td>\n",
              "      <td>15.332805</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.551194</td>\n",
              "      <td>0.901265</td>\n",
              "      <td>1.205963</td>\n",
              "      <td>1.269742</td>\n",
              "      <td>1.040393</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.568101</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.363856</td>\n",
              "      <td>2.054306</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.253727</td>\n",
              "      <td>0.056773</td>\n",
              "      <td>-0.654220</td>\n",
              "      <td>-0.135472</td>\n",
              "      <td>0.240176</td>\n",
              "      <td>0.284358</td>\n",
              "      <td>0.301712</td>\n",
              "      <td>0.030432</td>\n",
              "      <td>0.982796</td>\n",
              "      <td>1.079335</td>\n",
              "      <td>-0.491665</td>\n",
              "      <td>0.192161</td>\n",
              "      <td>-0.459778</td>\n",
              "      <td>-0.532213</td>\n",
              "      <td>0.090021</td>\n",
              "      <td>0.661293</td>\n",
              "      <td>-0.517795</td>\n",
              "      <td>-0.221878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.717841</td>\n",
              "      <td>17.493813</td>\n",
              "      <td>21.625214</td>\n",
              "      <td>19.902271</td>\n",
              "      <td>16.396116</td>\n",
              "      <td>18.681686</td>\n",
              "      <td>9.622036</td>\n",
              "      <td>15.566667</td>\n",
              "      <td>16.872272</td>\n",
              "      <td>2.891501</td>\n",
              "      <td>19.356615</td>\n",
              "      <td>5.649988</td>\n",
              "      <td>12.538577</td>\n",
              "      <td>12.147779</td>\n",
              "      <td>13.704709</td>\n",
              "      <td>14.683738</td>\n",
              "      <td>0.0</td>\n",
              "      <td>123.392120</td>\n",
              "      <td>63.841248</td>\n",
              "      <td>99.410301</td>\n",
              "      <td>41.583771</td>\n",
              "      <td>54.305538</td>\n",
              "      <td>70.684692</td>\n",
              "      <td>57.173073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.758835</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.323105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>81.449501</td>\n",
              "      <td>-1.036402</td>\n",
              "      <td>-0.499439</td>\n",
              "      <td>-0.495896</td>\n",
              "      <td>0.526013</td>\n",
              "      <td>0.085208</td>\n",
              "      <td>0.504849</td>\n",
              "      <td>0.274258</td>\n",
              "      <td>0.585883</td>\n",
              "      <td>0.109990</td>\n",
              "      <td>0.015748</td>\n",
              "      <td>-0.297879</td>\n",
              "      <td>-1.042105</td>\n",
              "      <td>0.454494</td>\n",
              "      <td>-0.408630</td>\n",
              "      <td>-1.007253</td>\n",
              "      <td>-0.327223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.525606</td>\n",
              "      <td>7.729619</td>\n",
              "      <td>12.469887</td>\n",
              "      <td>8.893851</td>\n",
              "      <td>4.215873</td>\n",
              "      <td>9.874451</td>\n",
              "      <td>8.484407</td>\n",
              "      <td>7.388212</td>\n",
              "      <td>3.339989</td>\n",
              "      <td>5.688490</td>\n",
              "      <td>9.019494</td>\n",
              "      <td>9.228015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.112737</td>\n",
              "      <td>0.272345</td>\n",
              "      <td>11.246958</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.650431</td>\n",
              "      <td>0.686527</td>\n",
              "      <td>2.626606</td>\n",
              "      <td>0.403262</td>\n",
              "      <td>0.888433</td>\n",
              "      <td>1.759920</td>\n",
              "      <td>2.395559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.406031</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.275555</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.573129</td>\n",
              "      <td>0.259972</td>\n",
              "      <td>0.126702</td>\n",
              "      <td>-0.223310</td>\n",
              "      <td>-0.417592</td>\n",
              "      <td>0.119613</td>\n",
              "      <td>0.818704</td>\n",
              "      <td>0.665083</td>\n",
              "      <td>0.125510</td>\n",
              "      <td>-0.526279</td>\n",
              "      <td>-0.132033</td>\n",
              "      <td>-0.567343</td>\n",
              "      <td>0.076380</td>\n",
              "      <td>0.186313</td>\n",
              "      <td>-0.024057</td>\n",
              "      <td>0.067190</td>\n",
              "      <td>0.451633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.746030</td>\n",
              "      <td>6.962137</td>\n",
              "      <td>8.937541</td>\n",
              "      <td>10.381464</td>\n",
              "      <td>12.112238</td>\n",
              "      <td>9.934958</td>\n",
              "      <td>11.205392</td>\n",
              "      <td>5.096816</td>\n",
              "      <td>2.349538</td>\n",
              "      <td>8.490510</td>\n",
              "      <td>8.046690</td>\n",
              "      <td>6.587426</td>\n",
              "      <td>6.181075</td>\n",
              "      <td>9.011739</td>\n",
              "      <td>6.192536</td>\n",
              "      <td>5.939188</td>\n",
              "      <td>0.0</td>\n",
              "      <td>123.392120</td>\n",
              "      <td>63.841248</td>\n",
              "      <td>99.410301</td>\n",
              "      <td>41.583771</td>\n",
              "      <td>54.305538</td>\n",
              "      <td>70.684692</td>\n",
              "      <td>57.173073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>37.758835</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.323105</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>81.449501</td>\n",
              "      <td>-1.087654</td>\n",
              "      <td>-0.179319</td>\n",
              "      <td>0.102155</td>\n",
              "      <td>-0.504031</td>\n",
              "      <td>-0.043728</td>\n",
              "      <td>0.060955</td>\n",
              "      <td>-0.133286</td>\n",
              "      <td>0.089309</td>\n",
              "      <td>-0.069125</td>\n",
              "      <td>-0.879518</td>\n",
              "      <td>-0.286424</td>\n",
              "      <td>0.202886</td>\n",
              "      <td>0.067224</td>\n",
              "      <td>0.301671</td>\n",
              "      <td>0.068741</td>\n",
              "      <td>0.335711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10235</th>\n",
              "      <td>14.320944</td>\n",
              "      <td>9.797541</td>\n",
              "      <td>13.178794</td>\n",
              "      <td>9.223291</td>\n",
              "      <td>12.944340</td>\n",
              "      <td>8.568989</td>\n",
              "      <td>13.726956</td>\n",
              "      <td>5.387570</td>\n",
              "      <td>6.286666</td>\n",
              "      <td>6.619838</td>\n",
              "      <td>11.021467</td>\n",
              "      <td>10.266586</td>\n",
              "      <td>11.548632</td>\n",
              "      <td>5.286120</td>\n",
              "      <td>8.096165</td>\n",
              "      <td>9.989305</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.152220</td>\n",
              "      <td>-0.803319</td>\n",
              "      <td>-0.895731</td>\n",
              "      <td>0.608661</td>\n",
              "      <td>0.745475</td>\n",
              "      <td>0.378171</td>\n",
              "      <td>0.808447</td>\n",
              "      <td>0.997398</td>\n",
              "      <td>-0.370447</td>\n",
              "      <td>0.598768</td>\n",
              "      <td>-0.119378</td>\n",
              "      <td>-0.415833</td>\n",
              "      <td>-0.221954</td>\n",
              "      <td>0.182336</td>\n",
              "      <td>-0.350657</td>\n",
              "      <td>-0.402497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10236</th>\n",
              "      <td>7.283374</td>\n",
              "      <td>2.749018</td>\n",
              "      <td>13.989727</td>\n",
              "      <td>9.975429</td>\n",
              "      <td>7.808156</td>\n",
              "      <td>13.104544</td>\n",
              "      <td>15.540765</td>\n",
              "      <td>10.433757</td>\n",
              "      <td>7.728669</td>\n",
              "      <td>12.493134</td>\n",
              "      <td>12.843208</td>\n",
              "      <td>10.414503</td>\n",
              "      <td>15.673471</td>\n",
              "      <td>3.529014</td>\n",
              "      <td>12.453532</td>\n",
              "      <td>8.305533</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.538382</td>\n",
              "      <td>-0.594273</td>\n",
              "      <td>0.352540</td>\n",
              "      <td>-0.120017</td>\n",
              "      <td>-0.149096</td>\n",
              "      <td>0.541178</td>\n",
              "      <td>0.812760</td>\n",
              "      <td>0.923694</td>\n",
              "      <td>-0.646873</td>\n",
              "      <td>-0.045234</td>\n",
              "      <td>-0.590301</td>\n",
              "      <td>-0.646457</td>\n",
              "      <td>-0.485697</td>\n",
              "      <td>-0.180688</td>\n",
              "      <td>-0.499646</td>\n",
              "      <td>0.242648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10237</th>\n",
              "      <td>14.668739</td>\n",
              "      <td>17.153061</td>\n",
              "      <td>18.849628</td>\n",
              "      <td>16.924801</td>\n",
              "      <td>16.816706</td>\n",
              "      <td>18.075962</td>\n",
              "      <td>23.956923</td>\n",
              "      <td>9.297574</td>\n",
              "      <td>15.733624</td>\n",
              "      <td>23.805605</td>\n",
              "      <td>15.216730</td>\n",
              "      <td>13.481298</td>\n",
              "      <td>15.414532</td>\n",
              "      <td>13.886168</td>\n",
              "      <td>24.709255</td>\n",
              "      <td>33.960194</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.732812</td>\n",
              "      <td>-1.283967</td>\n",
              "      <td>0.197211</td>\n",
              "      <td>0.274080</td>\n",
              "      <td>0.434829</td>\n",
              "      <td>-0.049085</td>\n",
              "      <td>1.052436</td>\n",
              "      <td>0.503341</td>\n",
              "      <td>-0.963324</td>\n",
              "      <td>0.844328</td>\n",
              "      <td>-0.246956</td>\n",
              "      <td>-0.600262</td>\n",
              "      <td>0.502401</td>\n",
              "      <td>-0.496158</td>\n",
              "      <td>-0.838488</td>\n",
              "      <td>-0.496424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10238</th>\n",
              "      <td>8.766872</td>\n",
              "      <td>4.232332</td>\n",
              "      <td>16.911827</td>\n",
              "      <td>12.412842</td>\n",
              "      <td>10.520018</td>\n",
              "      <td>15.297036</td>\n",
              "      <td>18.743952</td>\n",
              "      <td>16.318684</td>\n",
              "      <td>6.590806</td>\n",
              "      <td>11.281427</td>\n",
              "      <td>15.499956</td>\n",
              "      <td>10.263979</td>\n",
              "      <td>20.670725</td>\n",
              "      <td>6.094724</td>\n",
              "      <td>11.486252</td>\n",
              "      <td>19.297413</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.188583</td>\n",
              "      <td>0.436562</td>\n",
              "      <td>-0.090641</td>\n",
              "      <td>-0.102353</td>\n",
              "      <td>0.326288</td>\n",
              "      <td>-0.304644</td>\n",
              "      <td>0.643151</td>\n",
              "      <td>0.565106</td>\n",
              "      <td>-0.146242</td>\n",
              "      <td>0.376583</td>\n",
              "      <td>-0.292953</td>\n",
              "      <td>-0.068282</td>\n",
              "      <td>0.389697</td>\n",
              "      <td>1.096096</td>\n",
              "      <td>-0.521289</td>\n",
              "      <td>-0.078485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10239</th>\n",
              "      <td>21.133024</td>\n",
              "      <td>11.799814</td>\n",
              "      <td>20.579357</td>\n",
              "      <td>29.202312</td>\n",
              "      <td>13.692835</td>\n",
              "      <td>15.879473</td>\n",
              "      <td>31.298618</td>\n",
              "      <td>28.274750</td>\n",
              "      <td>7.714289</td>\n",
              "      <td>20.559334</td>\n",
              "      <td>15.310165</td>\n",
              "      <td>7.743214</td>\n",
              "      <td>0.138265</td>\n",
              "      <td>16.522406</td>\n",
              "      <td>19.244167</td>\n",
              "      <td>17.022907</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.315783</td>\n",
              "      <td>-0.913160</td>\n",
              "      <td>-0.369992</td>\n",
              "      <td>0.161561</td>\n",
              "      <td>0.272147</td>\n",
              "      <td>0.133743</td>\n",
              "      <td>1.711050</td>\n",
              "      <td>0.876007</td>\n",
              "      <td>-1.007004</td>\n",
              "      <td>0.736928</td>\n",
              "      <td>-0.363371</td>\n",
              "      <td>-0.636034</td>\n",
              "      <td>1.100344</td>\n",
              "      <td>-0.647478</td>\n",
              "      <td>-1.065035</td>\n",
              "      <td>-0.036072</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10240 rows × 48 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0          1          2  ...         n         o         p\n",
              "0       7.211837   4.940129   6.629362  ... -0.495680 -0.603125  0.482074\n",
              "1       8.685092  24.993542   8.810983  ...  0.661293 -0.517795 -0.221878\n",
              "2      10.717841  17.493813  21.625214  ... -0.408630 -1.007253 -0.327223\n",
              "3       7.525606   7.729619  12.469887  ... -0.024057  0.067190  0.451633\n",
              "4       8.746030   6.962137   8.937541  ...  0.301671  0.068741  0.335711\n",
              "...          ...        ...        ...  ...       ...       ...       ...\n",
              "10235  14.320944   9.797541  13.178794  ...  0.182336 -0.350657 -0.402497\n",
              "10236   7.283374   2.749018  13.989727  ... -0.180688 -0.499646  0.242648\n",
              "10237  14.668739  17.153061  18.849628  ... -0.496158 -0.838488 -0.496424\n",
              "10238   8.766872   4.232332  16.911827  ...  1.096096 -0.521289 -0.078485\n",
              "10239  21.133024  11.799814  20.579357  ... -0.647478 -1.065035 -0.036072\n",
              "\n",
              "[10240 rows x 48 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "LOktPhYpJNEg",
        "outputId": "b720609c-93fc-4540-ae27-adc0410b3f5c"
      },
      "source": [
        "d_y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>barely-true</th>\n",
              "      <th>false</th>\n",
              "      <th>half-true</th>\n",
              "      <th>mostly-true</th>\n",
              "      <th>pants-fire</th>\n",
              "      <th>true</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10237</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10238</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10239</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10240</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10241</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10240 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       barely-true  false  half-true  mostly-true  pants-fire  true\n",
              "0                0      1          0            0           0     0\n",
              "1                0      0          1            0           0     0\n",
              "2                0      0          0            1           0     0\n",
              "3                0      1          0            0           0     0\n",
              "4                0      0          1            0           0     0\n",
              "...            ...    ...        ...          ...         ...   ...\n",
              "10237            0      0          0            1           0     0\n",
              "10238            0      0          0            1           0     0\n",
              "10239            0      0          1            0           0     0\n",
              "10240            0      1          0            0           0     0\n",
              "10241            0      0          0            0           1     0\n",
              "\n",
              "[10240 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiZg0gMfAlem"
      },
      "source": [
        "## Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rne4ocZmAmlb",
        "outputId": "9046139e-8738-4506-9da1-a5931e9f8959"
      },
      "source": [
        "model.fit(e_X_train, d_y_train,\n",
        "          epochs=1000,\n",
        "          batch_size=8,\n",
        "          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7714\n",
            "Epoch 2/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7580\n",
            "Epoch 3/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7581\n",
            "Epoch 4/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7579\n",
            "Epoch 5/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7573\n",
            "Epoch 6/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7572\n",
            "Epoch 7/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7573\n",
            "Epoch 8/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7572\n",
            "Epoch 9/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 10/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 11/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7571\n",
            "Epoch 12/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 13/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 14/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 15/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 16/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 17/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 18/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 19/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 20/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 21/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 22/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 23/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 24/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 25/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 26/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 27/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 28/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 29/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 30/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 31/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 32/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 33/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 34/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 35/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 36/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 37/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 38/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 39/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 40/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 41/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 42/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 43/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 44/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 45/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 46/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 47/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 48/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 49/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 50/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 51/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 52/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 53/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 54/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 55/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 56/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 57/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 58/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 59/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 60/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 61/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 62/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 63/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 64/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 65/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7568\n",
            "Epoch 66/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 67/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 68/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 69/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 70/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 71/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 72/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 73/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 74/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 75/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 76/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 77/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 78/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 79/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 80/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 81/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 82/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 83/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 84/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 85/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 86/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 87/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 88/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7571\n",
            "Epoch 89/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 90/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 91/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 92/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 93/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 94/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 95/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 96/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 97/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 98/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 99/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 100/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 101/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 102/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 103/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 104/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 105/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 106/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 107/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 108/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 109/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 110/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 111/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 112/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 113/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 114/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 115/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 116/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 117/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 118/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 119/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 120/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 121/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 122/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 123/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 124/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 125/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 126/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 127/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 128/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 129/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 130/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 131/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 132/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 133/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 134/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 135/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 136/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 137/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 138/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 139/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 140/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 141/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 142/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 143/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 144/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 145/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 146/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 147/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 148/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 149/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 150/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 151/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 152/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 153/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 154/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 155/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 156/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 157/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 158/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 159/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 160/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 161/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 162/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 163/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 164/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 165/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 166/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 167/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 168/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 169/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 170/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 171/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 172/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 173/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 174/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 175/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 176/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 177/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 178/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 179/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 180/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 181/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 182/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 183/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 184/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 185/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 186/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 187/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 188/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 189/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 190/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 191/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 192/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 193/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 194/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 195/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 196/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 197/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 198/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 199/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 200/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 201/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 202/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 203/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 204/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 205/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 206/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 207/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 208/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 209/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 210/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 211/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 212/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 213/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 214/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 215/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 216/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 217/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 218/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 219/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 220/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 221/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 222/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 223/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 224/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 225/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 226/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 227/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 228/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 229/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 230/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 231/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 232/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 233/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 234/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 235/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 236/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 237/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 238/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 239/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 240/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 241/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 242/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 243/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 244/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 245/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 246/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 247/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 248/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 249/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 250/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 251/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 252/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 253/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 254/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 255/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 256/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 257/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 258/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 259/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 260/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 261/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 262/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 263/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 264/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 265/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 266/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 267/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 268/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 269/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 270/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 271/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 272/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 273/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 274/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 275/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 276/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 277/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 278/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 279/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 280/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 281/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 282/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 283/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 284/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 285/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 286/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 287/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 288/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 289/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 290/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 291/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 292/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 293/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 294/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 295/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 296/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 297/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 298/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 299/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 300/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 301/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 302/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 303/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 304/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 305/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 306/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7568\n",
            "Epoch 307/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 308/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 309/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 310/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 311/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 312/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 313/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 314/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 315/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 316/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 317/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 318/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 319/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 320/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 321/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 322/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 323/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 324/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 325/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 326/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 327/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 328/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 329/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 330/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 331/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 332/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 333/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 334/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 335/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 336/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 337/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 338/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 339/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 340/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 341/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 342/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 343/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 344/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 345/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 346/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 347/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 348/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 349/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 350/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 351/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 352/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 353/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 354/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 355/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 356/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 357/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 358/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 359/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 360/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 361/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 362/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 363/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 364/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 365/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 366/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 367/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 368/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 369/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 370/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 371/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 372/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 373/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 374/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 375/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 376/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 377/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 378/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 379/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 380/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 381/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 382/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 383/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 384/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 385/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 386/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 387/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 388/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 389/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 390/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 391/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 392/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 393/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 394/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 395/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 396/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 397/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 398/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 399/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 400/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 401/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 402/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 403/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 404/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 405/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 406/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 407/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 408/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 409/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 410/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 411/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 412/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 413/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 414/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 415/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 416/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 417/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 418/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 419/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 420/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 421/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 422/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 423/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 424/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 425/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 426/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 427/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 428/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 429/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 430/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 431/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 432/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 433/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 434/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 435/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 436/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 437/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 438/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 439/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 440/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 441/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 442/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 443/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 444/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 445/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 446/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 447/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 448/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 449/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 450/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 451/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 452/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 453/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 454/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 455/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 456/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 457/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 458/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 459/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 460/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 461/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 462/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 463/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 464/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 465/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 466/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 467/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 468/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 469/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 470/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 471/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 472/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 473/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 474/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 475/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 476/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 477/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 478/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 479/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 480/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 481/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 482/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 483/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 484/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 485/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 486/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 487/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 488/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 489/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 490/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 491/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 492/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 493/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 494/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 495/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 496/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 497/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 498/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 499/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 500/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 501/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 502/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 503/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 504/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 505/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 506/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 507/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 508/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 509/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 510/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 511/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 512/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 513/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 514/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 515/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 516/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 517/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 518/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 519/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 520/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 521/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 522/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 523/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 524/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 525/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 526/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 527/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 528/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 529/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 530/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 531/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 532/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 533/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 534/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 535/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 536/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 537/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 538/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 539/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 540/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 541/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 542/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 543/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 544/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 545/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 546/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 547/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 548/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 549/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 550/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 551/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 552/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 553/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 554/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 555/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 556/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 557/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 558/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 559/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 560/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 561/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 562/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 563/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 564/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 565/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 566/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 567/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 568/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 569/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 570/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 571/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 572/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 573/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 574/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 575/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 576/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 577/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 578/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 579/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 580/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 581/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 582/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 583/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 584/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 585/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 586/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 587/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 588/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 589/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 590/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 591/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 592/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 593/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 594/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 595/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 596/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 597/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 598/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 599/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 600/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 601/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 602/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 603/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 604/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 605/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 606/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 607/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 608/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 609/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 610/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 611/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 612/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 613/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 614/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 615/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 616/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 617/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 618/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 619/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 620/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 621/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 622/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 623/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 624/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 625/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 626/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 627/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 628/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 629/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 630/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 631/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 632/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 633/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 634/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 635/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 636/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 637/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 638/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7568\n",
            "Epoch 639/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 640/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 641/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 642/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 643/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 644/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 645/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 646/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 647/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 648/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 649/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 650/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 651/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 652/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 653/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 654/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 655/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 656/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 657/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 658/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 659/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 660/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 661/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 662/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 663/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 664/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 665/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 666/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 667/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 668/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 669/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 670/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 671/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 672/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 673/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 674/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 675/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 676/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 677/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 678/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 679/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 680/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 681/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 682/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 683/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 684/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 685/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 686/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 687/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 688/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 689/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 690/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 691/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 692/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 693/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 694/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 695/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 696/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 697/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 698/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 699/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 700/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 701/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 702/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 703/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 704/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 705/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 706/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 707/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 708/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 709/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 710/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 711/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 712/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 713/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 714/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 715/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 716/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 717/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 718/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 719/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 720/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 721/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 722/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 723/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 724/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 725/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 726/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 727/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 728/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 729/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 730/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 731/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 732/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 733/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 734/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 735/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 736/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 737/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 738/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 739/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 740/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 741/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 742/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 743/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 744/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7568\n",
            "Epoch 745/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 746/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 747/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 748/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 749/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 750/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 751/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 752/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 753/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 754/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 755/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 756/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 757/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 758/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 759/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 760/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 761/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 762/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 763/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 764/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 765/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 766/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 767/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 768/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 769/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 770/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 771/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 772/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 773/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 774/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 775/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 776/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 777/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 778/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 779/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 780/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 781/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 782/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 783/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 784/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 785/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 786/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 787/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 788/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 789/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 790/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 791/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 792/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 793/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 794/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 795/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 796/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 797/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 798/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 799/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 800/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 801/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 802/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 803/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 804/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 805/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 806/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 807/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 808/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 809/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 810/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 811/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 812/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 813/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 814/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 815/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 816/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 817/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 818/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 819/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 820/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 821/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 822/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 823/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 824/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 825/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 826/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 827/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 828/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 829/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7568\n",
            "Epoch 830/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 831/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 832/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 833/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 834/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 835/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 836/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 837/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 838/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 839/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 840/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 841/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 842/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 843/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 844/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 845/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 846/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 847/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 848/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 849/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 850/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 851/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 852/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 853/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 854/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 855/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 856/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 857/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 858/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 859/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 860/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 861/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 862/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 863/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 864/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 865/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 866/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 867/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 868/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 869/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 870/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 871/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 872/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 873/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 874/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 875/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 876/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 877/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 878/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 879/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 880/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 881/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 882/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 883/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 884/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 885/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 886/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 887/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 888/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 889/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 890/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 891/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 892/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 893/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 894/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 895/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 896/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 897/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 898/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 899/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 900/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 901/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 902/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 903/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 904/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 905/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 906/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 907/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 908/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 909/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7569\n",
            "Epoch 910/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 911/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7568\n",
            "Epoch 912/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 913/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 914/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 915/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 916/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 917/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 918/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 919/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 920/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 921/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 922/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 923/1000\n",
            "1280/1280 [==============================] - 3s 2ms/step - loss: 1.7570\n",
            "Epoch 924/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 925/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 926/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 927/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 928/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 929/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 930/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 931/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 932/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 933/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 934/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 935/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 936/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 937/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 938/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 939/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 940/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 941/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 942/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 943/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 944/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 945/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 946/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 947/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 948/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 949/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 950/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 951/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 952/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 953/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 954/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 955/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 956/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 957/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 958/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 959/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 960/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 961/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 962/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 963/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 964/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 965/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 966/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 967/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 968/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 969/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 970/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 971/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 972/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 973/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 974/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 975/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 976/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 977/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 978/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 979/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 980/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 981/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 982/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 983/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 984/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 985/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 986/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 987/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 988/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 989/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 990/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 991/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 992/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 993/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 994/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 995/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 996/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 997/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 998/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7569\n",
            "Epoch 999/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n",
            "Epoch 1000/1000\n",
            "1280/1280 [==============================] - 2s 2ms/step - loss: 1.7570\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6d4d763a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOqdYjRBPzxJ"
      },
      "source": [
        "## Save & Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5xw71d2VUDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2eeba09-6244-4a14-eb6e-bd866747b449"
      },
      "source": [
        "#SAVE KERAS MODEL\n",
        "from tensorflow import keras\n",
        "model.save('/content/MODEL')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/MODEL/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYISPHIJaKZX"
      },
      "source": [
        "#ZIP MODEL\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "def zipdir(path, ziph):\n",
        "    # ziph is zipfile handle\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))\n",
        "\n",
        "\n",
        "zipf = zipfile.ZipFile('model.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('./MODEL/', zipf)\n",
        "zipf.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IG3zFSO7sFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1297b7cd-84e3-4306-bb7e-4102da02251c"
      },
      "source": [
        "#DOWNLOAD MODEL FROM GOOGLE SHARED DRIVE\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "gdd.download_file_from_google_drive(file_id='1tPWFaGKF7Ibiq-kP897Uh7Nk0Yix6jFX',\n",
        "                                  dest_path='./mlp_model.zip',\n",
        "                                  unzip=False)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 1tPWFaGKF7Ibiq-kP897Uh7Nk0Yix6jFX into ./mlp_model.zip... Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WvxEp_t2oc5"
      },
      "source": [
        "#UNZIP\n",
        "import zipfile\n",
        "with zipfile.ZipFile('/content/mlp_model.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojQP02R1XW01"
      },
      "source": [
        "#LOAD MODEL\n",
        "from tensorflow import keras\n",
        "mlp_model = keras.models.load_model('/content/MODEL/')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ9fE85Bk-3_"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASr6l59PlBaM"
      },
      "source": [
        "import numpy as np\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from nlp_sensationalism.sensaEncoder import SensaEncoder\n",
        "\n",
        "def MLPInference(title, model):\n",
        "    bcs = BlastoffContentStatistics()\n",
        "    sensaEncoder = SensaEncoder()\n",
        "    cv = Context_Veracity()\n",
        "    labels = ['barely-true', 'false', 'half-true', 'mostly-true', 'pants-fire', 'true']\n",
        "\n",
        "    # Sensationalism\n",
        "    \n",
        "    a = sensaEncoder.encodeText(title, dimension=16).numpy()\n",
        "    sdf = pd.DataFrame()\n",
        "    sdf = sdf.append({'a':a[0],'b':a[1],'c':a[2],'d':a[3],'e':a[4],\n",
        "                      'f':a[5],'g':a[6],'h':a[7],'i':a[8],'j':a[9],\n",
        "                      'k':a[10],'l':a[11],'m':a[12],'n':a[13],'o':a[14],\n",
        "                      'p':a[15]}, ignore_index=True)\n",
        "    bse_e_X_train = sdf\n",
        "\n",
        "    X_train = pd.DataFrame(data=[title], columns=['Statement'])\n",
        "    # print(X_train)\n",
        "    # Content Statistics\n",
        "    bcs_e_X_train = bcs.encode(X_train)\n",
        "\n",
        "    # # Context Veracity\n",
        "    bcv_e_X_train = cv.encode(X_train)\n",
        "    \n",
        "    # # Concatenate\n",
        "    bcs_e_X_train = pd.DataFrame(data=bcs_e_X_train)\n",
        "    bcv_e_X_train = pd.DataFrame(data=bcv_e_X_train)\n",
        "    # bse_e_X_train = pd.DataFrame(data=bse_e_X_train)\n",
        "\n",
        "    e_X_train = pd.concat([bcs_e_X_train, bcv_e_X_train, bse_e_X_train], axis=1)\n",
        "\n",
        "    pred = model.predict(e_X_train)\n",
        "    clear_output()\n",
        "\n",
        "    return labels[np.argmax(pred[0])]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OTH7LqSulygC",
        "outputId": "c59bbc07-0f30-4c73-b0ab-0925bf34f265"
      },
      "source": [
        "MLPInference('Media fawns over Biden\\'s Cabinet rollout, describes being rescued from this craziness by superheroes', model=mlp_model)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'half-true'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfiCGP9zNn4P"
      },
      "source": [
        "# How did we train our models?\n",
        "\n",
        "Han-Wei Lin [Content Statistics] -> https://colab.research.google.com/drive/1pOhnmm6X_J4t__2VxSISoDGTtWa_gZ5-?usp=sharing\n",
        "\n",
        "Roger Navarro [Sensationalism] -> https://github.com/roanacla/ML_Sensationalism/blob/main/Sensationalism.ipynb\n",
        "\n",
        "Sheetal Narvekar [Context Veracity] -> https://github.com/snarvekark/Veracity_Factor/blob/master/Context_Veracity_Main.ipynb"
      ]
    }
  ]
}